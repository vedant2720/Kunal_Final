{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txsPaMJWY3x0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import gc\n",
        "import cv2\n",
        "import timm\n",
        "from timm.models.vision_transformer import VisionTransformer\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# GPU Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Memory management functions\n",
        "def free_memory():\n",
        "    \"\"\"Free memory to avoid OOM errors\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def print_gpu_memory():\n",
        "    \"\"\"Print available and allocated GPU memory\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "        # Calculate available memory\n",
        "        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()\n",
        "        print(f\"GPU memory available: {free_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Custom dataset class for videos\n",
        "class DeepFakeVideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, labels, num_frames=16, transform=None, max_frame_retries=3):\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.num_frames = num_frames\n",
        "        self.transform = transform\n",
        "        self.max_frame_retries = max_frame_retries\n",
        "\n",
        "        # Pre-check videos and filter out problematic ones\n",
        "        self._filter_valid_videos()\n",
        "\n",
        "    def _filter_valid_videos(self):\n",
        "        \"\"\"Pre-check videos and remove invalid ones.\"\"\"\n",
        "        valid_videos = []\n",
        "        valid_labels = []\n",
        "\n",
        "        for i, video_path in enumerate(tqdm(self.video_paths, desc=\"Validating videos\")):\n",
        "            try:\n",
        "                cap = cv2.VideoCapture(video_path)\n",
        "                if not cap.isOpened():\n",
        "                    print(f\"Warning: Could not open video {video_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Check if video has frames\n",
        "                ret, _ = cap.read()\n",
        "                if not ret:\n",
        "                    print(f\"Warning: No frames in video {video_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Video is valid\n",
        "                valid_videos.append(video_path)\n",
        "                valid_labels.append(self.labels[i])\n",
        "\n",
        "                cap.release()\n",
        "            except Exception as e:\n",
        "                print(f\"Error validating video {video_path}: {e}\")\n",
        "\n",
        "        # Update videos and labels\n",
        "        self.video_paths = valid_videos\n",
        "        self.labels = valid_labels\n",
        "        print(f\"Kept {len(valid_videos)} valid videos out of {len(self.video_paths)} total\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def extract_frames(self, video_path):\n",
        "        frames = []\n",
        "        retry_count = 0\n",
        "\n",
        "        while retry_count < self.max_frame_retries:\n",
        "            try:\n",
        "                cap = cv2.VideoCapture(video_path)\n",
        "                if not cap.isOpened():\n",
        "                    raise ValueError(f\"Cannot open video file: {video_path}\")\n",
        "\n",
        "                # Get total number of frames\n",
        "                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "                if total_frames <= 0:\n",
        "                    raise ValueError(f\"Video has no frames: {video_path}\")\n",
        "\n",
        "                # Calculate sampling interval to get num_frames evenly distributed\n",
        "                if total_frames <= self.num_frames:\n",
        "                    # If video has fewer frames than needed, duplicate frames\n",
        "                    indices = list(range(total_frames)) * (self.num_frames // total_frames + 1)\n",
        "                    indices = indices[:self.num_frames]\n",
        "                else:\n",
        "                    # Sample frames evenly\n",
        "                    indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
        "\n",
        "                for idx in indices:\n",
        "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "                    ret, frame = cap.read()\n",
        "                    if ret:\n",
        "                        # Convert BGR to RGB\n",
        "                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                        if self.transform:\n",
        "                            frame = self.transform(image=frame)[\"image\"]\n",
        "                        frames.append(frame)\n",
        "                    else:\n",
        "                        # If frame read fails, add a blank frame\n",
        "                        blank_frame = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "                        if self.transform:\n",
        "                            blank_frame = self.transform(image=blank_frame)[\"image\"]\n",
        "                        frames.append(blank_frame)\n",
        "\n",
        "                cap.release()\n",
        "\n",
        "                # If we got all frames successfully, break out of retry loop\n",
        "                if len(frames) == self.num_frames:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting frames (attempt {retry_count+1}): {e}\")\n",
        "                retry_count += 1\n",
        "\n",
        "                # Clear frames and retry\n",
        "                frames = []\n",
        "                continue\n",
        "\n",
        "            finally:\n",
        "                # Make sure to release the capture object\n",
        "                if 'cap' in locals() and cap is not None:\n",
        "                    cap.release()\n",
        "\n",
        "        # If we still don't have enough frames after all retries, create dummy frames\n",
        "        if len(frames) < self.num_frames:\n",
        "            missing_frames = self.num_frames - len(frames)\n",
        "            print(f\"Warning: Using {missing_frames} dummy frames for {video_path}\")\n",
        "\n",
        "            blank_frame = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            for _ in range(missing_frames):\n",
        "                if self.transform:\n",
        "                    dummy = self.transform(image=blank_frame)[\"image\"]\n",
        "                    frames.append(dummy)\n",
        "                else:\n",
        "                    # Convert numpy to tensor if no transform\n",
        "                    dummy = torch.from_numpy(blank_frame.transpose(2, 0, 1)).float() / 255.0\n",
        "                    frames.append(dummy)\n",
        "\n",
        "        # Stack frames into a tensor\n",
        "        frames = torch.stack(frames)\n",
        "        return frames\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            frames = self.extract_frames(video_path)\n",
        "            return frames, torch.tensor(label, dtype=torch.long)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading video at index {idx}, path: {video_path}, Error: {e}\")\n",
        "            # Return a dummy tensor and the label\n",
        "            dummy_frames = torch.zeros((self.num_frames, 3, 224, 224))\n",
        "            return dummy_frames, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Vision Transformer for Video - Fixed Implementation\n",
        "class ViTVideoClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2, num_frames=16, pretrained=True, memory_efficient=True):\n",
        "        super(ViTVideoClassifier, self).__init__()\n",
        "\n",
        "        # Use a smaller ViT model for memory efficiency if needed\n",
        "        if memory_efficient:\n",
        "            # Use ViT-Small instead of ViT-Base or larger versions\n",
        "            self.vit_encoder = timm.create_model('vit_small_patch16_224', pretrained=pretrained)\n",
        "            embed_dim = 384  # ViT-Small embedding dimension\n",
        "        else:\n",
        "            # Use ViT-Base\n",
        "            self.vit_encoder = timm.create_model('vit_base_patch16_224', pretrained=pretrained)\n",
        "            embed_dim = 768  # ViT-Base embedding dimension\n",
        "\n",
        "        # Remove the classification head\n",
        "        self.vit_encoder.head = nn.Identity()\n",
        "\n",
        "        # Create a proper temporal encoder with transformer architecture\n",
        "        # First create the encoder layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=8,\n",
        "            dim_feedforward=1024,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Then create the transformer encoder with multiple layers\n",
        "        self.temporal_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer=encoder_layer,\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "        # Separate CNN feature extractor for temporal features if needed\n",
        "        self.temporal_cnn = nn.Sequential(\n",
        "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
        "        )\n",
        "\n",
        "        # Classifier (Decoder) - Simplified to work with the transformer outputs\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Positional encoding for frames\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n",
        "\n",
        "        # Initialize positional embeddings\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, num_frames, channels, height, width]\n",
        "        batch_size, num_frames, c, h, w = x.shape\n",
        "\n",
        "        # Process each frame with ViT encoder\n",
        "        frame_features = []\n",
        "        for i in range(num_frames):\n",
        "            # Extract features for the current frame\n",
        "            features = self.vit_encoder(x[:, i])  # [batch_size, embed_dim]\n",
        "            frame_features.append(features)\n",
        "\n",
        "        # Stack frame features along the temporal dimension\n",
        "        x = torch.stack(frame_features, dim=1)  # [batch_size, num_frames, embed_dim]\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Apply temporal encoder (Transformer)\n",
        "        x = self.temporal_encoder(x)\n",
        "\n",
        "        # Global temporal pooling (mean pooling across frames)\n",
        "        x = torch.mean(x, dim=1)  # [batch_size, embed_dim]\n",
        "\n",
        "        # Classification (Decoder)\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for frames, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update learning rate\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Collect statistics\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Free up memory\n",
        "        del frames, labels, outputs, loss, preds\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Mixed precision training function\n",
        "def train_epoch_mixed_precision(model, dataloader, criterion, optimizer, scheduler, device, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for frames, labels in tqdm(dataloader, desc=\"Training (mixed precision)\"):\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision - handle both older and newer PyTorch versions\n",
        "        try:\n",
        "            # For newer PyTorch versions\n",
        "            from torch.amp import autocast\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(frames)\n",
        "                loss = criterion(outputs, labels)\n",
        "        except (ImportError, AttributeError):\n",
        "            # For older PyTorch versions\n",
        "            from torch.cuda.amp import autocast\n",
        "            with autocast():\n",
        "                outputs = model(frames)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize with scaler\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update learning rate\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Collect statistics\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Free up memory\n",
        "        del frames, labels, outputs, loss, preds\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Validation function\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Collect statistics\n",
        "            running_loss += loss.item() * frames.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Free up memory\n",
        "            del frames, labels, outputs, loss, preds\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc, report, conf_matrix\n",
        "\n",
        "# Main training loop with memory optimization\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, checkpoint_path=\"model_checkpoints\", scaler=None):\n",
        "    # Create checkpoint directory if it doesn't exist\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Print GPU memory usage before training\n",
        "        print(\"Before training:\")\n",
        "        print_gpu_memory()\n",
        "\n",
        "        # Train for one epoch\n",
        "        if scaler is not None:\n",
        "            train_loss, train_acc = train_epoch_mixed_precision(model, train_loader, criterion, optimizer, scheduler, device, scaler)\n",
        "        else:\n",
        "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "\n",
        "        # Free memory\n",
        "        free_memory()\n",
        "\n",
        "        # Print GPU memory usage after training\n",
        "        print(\"After training, before validation:\")\n",
        "        print_gpu_memory()\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, report, conf_matrix = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Free memory\n",
        "        free_memory()\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        # print(\"\\nClassification Report:\")\n",
        "        # print(report)\n",
        "\n",
        "        # Save statistics\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Save model if it's the best so far\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            print(f\"New best validation accuracy: {best_val_acc:.4f}\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_loss': val_loss,\n",
        "            }, os.path.join(checkpoint_path, f\"best_model_epoch_{epoch+1}.pth\"))\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_loss': val_loss,\n",
        "            }, os.path.join(checkpoint_path, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
        "\n",
        "        print(f\"GPU memory after epoch {epoch+1}:\")\n",
        "        print_gpu_memory()\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train')\n",
        "    plt.plot(val_losses, label='Validation')\n",
        "    plt.title('Loss over epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train')\n",
        "    plt.plot(val_accs, label='Validation')\n",
        "    plt.title('Accuracy over epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs\n",
        "\n",
        "# Main execution code\n",
        "def main():\n",
        "    # Hyperparameters optimized for Celeb-DF-v2 dataset with 15GB GPU\n",
        "    BATCH_SIZE = 2  # Very small batch size to save memory\n",
        "    NUM_FRAMES = 6  # Reduced for memory efficiency\n",
        "    NUM_EPOCHS = 10  # As requested\n",
        "    LEARNING_RATE = 2e-5  # Slightly lower learning rate for stability\n",
        "    NUM_WORKERS = 2  # Adjust based on your system\n",
        "\n",
        "    # Enable mixed precision for better memory efficiency\n",
        "    USE_MIXED_PRECISION = True\n",
        "\n",
        "    # Dataset balancing and limiting (in case the dataset is too large)\n",
        "    MAX_VIDEOS_PER_CLASS = 1000  # Limit videos per class if needed to fit in memory\n",
        "    BALANCED_DATASET = True  # Ensure equal number of real/fake samples\n",
        "\n",
        "    # Progress tracking on console\n",
        "    print(\"Starting DeepFake detection pipeline...\")\n",
        "\n",
        "    # Data transformations\n",
        "    print(\"Setting up data transformations...\")\n",
        "    train_transforms = A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    val_transforms = A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    # Load dataset files directly from Celeb-DF-v2 dataset structure\n",
        "    data_path = \"/kaggle/input/1000-df/dataset\"\n",
        "    print(\"Loading dataset from:\", data_path)\n",
        "\n",
        "    # Celeb-DF typically has this structure:\n",
        "    # - Celeb-real (real videos)\n",
        "    # - Celeb-synthesis (fake/deepfake videos)\n",
        "    # - YouTube-real (additional real videos)\n",
        "\n",
        "    real_paths = []\n",
        "    fake_paths = []\n",
        "\n",
        "    # Function to recursively find all video files in a directory\n",
        "    def find_videos(directory, extensions=('.mp4', '.avi')):\n",
        "        video_paths = []\n",
        "        if not os.path.exists(directory):\n",
        "            print(f\"Directory not found: {directory}\")\n",
        "            return video_paths\n",
        "\n",
        "        print(f\"Scanning directory: {directory}\")\n",
        "        for root, _, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(extensions):\n",
        "                    video_paths.append(os.path.join(root, file))\n",
        "        return video_paths\n",
        "\n",
        "    # Different possible dataset structures for Celeb-DF-v2\n",
        "    possible_structures = [\n",
        "        # Standard structure\n",
        "        {\n",
        "            \"real_dirs\": [\"real\"],\n",
        "            \"fake_dirs\": [\"fake\"]\n",
        "        },\n",
        "        # Alternative structure\n",
        "        {\n",
        "            \"real_dirs\": [\"real\"],\n",
        "            \"fake_dirs\": [\"fake\"]\n",
        "        },\n",
        "        # Another possible structure\n",
        "        {\n",
        "            \"real_dirs\": [\"original\"],\n",
        "            \"fake_dirs\": [\"manipulated\", \"deepfake\"]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Try each structure until we find videos\n",
        "    for structure in possible_structures:\n",
        "        real_paths = []\n",
        "        fake_paths = []\n",
        "\n",
        "        # Check real directories\n",
        "        for real_dir in structure[\"real_dirs\"]:\n",
        "            dir_path = os.path.join(data_path, real_dir)\n",
        "            real_paths.extend(find_videos(dir_path))\n",
        "\n",
        "        # Check fake directories\n",
        "        for fake_dir in structure[\"fake_dirs\"]:\n",
        "            dir_path = os.path.join(data_path, fake_dir)\n",
        "            fake_paths.extend(find_videos(dir_path))\n",
        "\n",
        "        # If we found videos, break out of the loop\n",
        "        if real_paths and fake_paths:\n",
        "            print(f\"Found valid dataset structure with {len(real_paths)} real videos and {len(fake_paths)} fake videos\")\n",
        "            break\n",
        "\n",
        "    # If we still don't have videos, try scanning the entire directory\n",
        "    if not real_paths or not fake_paths:\n",
        "        print(\"Could not find videos in expected directories. Scanning entire dataset directory...\")\n",
        "\n",
        "        # Look for directories or filename patterns that might indicate real/fake\n",
        "        all_videos = find_videos(data_path)\n",
        "\n",
        "        # Try to classify based on filename/path\n",
        "        for video_path in all_videos:\n",
        "            lower_path = video_path.lower()\n",
        "            if any(keyword in lower_path for keyword in [\"real\", \"original\", \"true\"]):\n",
        "                real_paths.append(video_path)\n",
        "            elif any(keyword in lower_path for keyword in [\"fake\", \"synthesis\", \"deepfake\", \"manipulated\"]):\n",
        "                fake_paths.append(video_path)\n",
        "            else:\n",
        "                # If can't determine, default to real\n",
        "                real_paths.append(video_path)\n",
        "\n",
        "        print(f\"After scanning, found {len(real_paths)} likely real videos and {len(fake_paths)} likely fake videos\")\n",
        "\n",
        "    # Balance and limit dataset if needed\n",
        "    if BALANCED_DATASET:\n",
        "        # Ensure equal number of real and fake samples\n",
        "        min_class_count = min(len(real_paths), len(fake_paths))\n",
        "\n",
        "        # Further limit if MAX_VIDEOS_PER_CLASS is specified\n",
        "        if MAX_VIDEOS_PER_CLASS > 0:\n",
        "            min_class_count = min(min_class_count, MAX_VIDEOS_PER_CLASS)\n",
        "\n",
        "        # Randomly sample from both classes\n",
        "        np.random.seed(42)  # For reproducibility\n",
        "        sampled_real_paths = np.random.choice(real_paths, min_class_count, replace=False)\n",
        "        sampled_fake_paths = np.random.choice(fake_paths, min_class_count, replace=False)\n",
        "\n",
        "        # Create balanced dataset\n",
        "        video_paths = list(sampled_real_paths) + list(sampled_fake_paths)\n",
        "        labels = [0] * len(sampled_real_paths) + [1] * len(sampled_fake_paths)  # 0 for real, 1 for fake\n",
        "    else:\n",
        "        # Use all videos (might be imbalanced)\n",
        "        video_paths = real_paths + fake_paths\n",
        "        labels = [0] * len(real_paths) + [1] * len(fake_paths)  # 0 for real, 1 for fake\n",
        "\n",
        "    print(f\"Total videos in dataset: {len(video_paths)}\")\n",
        "    print(f\"Real videos: {len(real_paths)} (using {np.sum(np.array(labels) == 0)})\")\n",
        "    print(f\"Fake videos: {len(fake_paths)} (using {np.sum(np.array(labels) == 1)})\")\n",
        "\n",
        "    # Split the data\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "        video_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Create datasets with progress information\n",
        "    print(f\"Creating training dataset with {len(train_paths)} videos...\")\n",
        "    train_dataset = DeepFakeVideoDataset(train_paths, train_labels, num_frames=NUM_FRAMES, transform=train_transforms)\n",
        "\n",
        "    print(f\"Creating validation dataset with {len(val_paths)} videos...\")\n",
        "    val_dataset = DeepFakeVideoDataset(val_paths, val_labels, num_frames=NUM_FRAMES, transform=val_transforms)\n",
        "\n",
        "    print(f\"Final dataset sizes - Training: {len(train_dataset)}, Validation: {len(val_dataset)}\")\n",
        "\n",
        "    # Create data loaders with appropriate memory settings\n",
        "    print(\"Creating data loaders...\")\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True if NUM_WORKERS > 0 else False,\n",
        "        drop_last=True  # Drop last incomplete batch to avoid size mismatch issues\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True if NUM_WORKERS > 0 else False,\n",
        "        drop_last=False  # Keep all validation samples\n",
        "    )\n",
        "\n",
        "    # Initialize model with proper seeding for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "    print(\"Initializing Vision Transformer model...\")\n",
        "    model = ViTVideoClassifier(num_classes=2, num_frames=NUM_FRAMES, pretrained=True, memory_efficient=True)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model architecture and parameter count\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n",
        "\n",
        "    # Optional: Print detailed model summary\n",
        "    # print(model)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    total_steps = len(train_loader) * NUM_EPOCHS\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=LEARNING_RATE, total_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Initialize mixed precision scaler if enabled\n",
        "    scaler = None\n",
        "    if USE_MIXED_PRECISION:\n",
        "        print(\"Using mixed precision training for better memory efficiency\")\n",
        "        try:\n",
        "            # Try the new import format first (for newer PyTorch versions)\n",
        "            from torch.amp import GradScaler\n",
        "            scaler = GradScaler()\n",
        "        except (ImportError, AttributeError):\n",
        "            # Fall back to the older import format\n",
        "            from torch.cuda.amp import GradScaler\n",
        "            scaler = GradScaler()\n",
        "\n",
        "        # Suppress FutureWarning about GradScaler deprecation\n",
        "        import warnings\n",
        "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "    # Initial GPU memory check\n",
        "    print(\"Initial GPU memory:\")\n",
        "    print_gpu_memory()\n",
        "\n",
        "    # Train the model\n",
        "    train_losses, train_accs, val_losses, val_accs = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS, device,\n",
        "        checkpoint_path=\"model_checkpoints\", scaler=scaler\n",
        "    )\n",
        "\n",
        "    # Final evaluation on validation set\n",
        "    model.load_state_dict(torch.load('./model_checkpoints/best_model_epoch_x.pth')['model_state_dict'])\n",
        "    val_loss, val_acc, report, conf_matrix = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(\"\\nFinal model performance:\")\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'val_acc': val_acc,\n",
        "    }, 'final_deepfake_detector.pth')\n",
        "\n",
        "    print(\"Training completed successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}